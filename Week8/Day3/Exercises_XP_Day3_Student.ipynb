{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c383420c",
      "metadata": {
        "id": "c383420c"
      },
      "source": [
        "# Exercises XP: Day 3 - BERT in Practice\n",
        "Follow the prompts below. Replace each TODO marker with your own code or explanation before executing the cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbae25ca",
      "metadata": {
        "id": "bbae25ca"
      },
      "source": [
        "## What you'll learn\n",
        "- How to tokenize text with BERT and understand special tokens.\n",
        "- How to run a pretrained sentiment pipeline.\n",
        "- How to build custom BERT-based sentiment and NER analyzers.\n",
        "- How to compare encoder (BERT) versus decoder (GPT) families.\n",
        "- How BERT supplies retrieval power inside a RAG stack.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2aa5bf0",
      "metadata": {
        "id": "c2aa5bf0"
      },
      "source": [
        "## What you will create\n",
        "- A fully tokenized sentence with visible IDs and special tokens.\n",
        "- A working sentiment pipeline powered by a fine-tuned DistilBERT model.\n",
        "- Custom helper classes for sentiment classification and NER.\n",
        "- A comparison table that contrasts BERT and GPT.\n",
        "- A written explanation of how BERT embeddings drive retrieval in RAG.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ed16a45",
      "metadata": {
        "id": "5ed16a45"
      },
      "source": [
        "> Mandatory preparation: watch \"PyTorch in 100 Seconds\" so the tensor outputs below feel intuitive."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4304ecfe",
      "metadata": {
        "id": "4304ecfe"
      },
      "source": [
        "## Exercise 1 - Tokenization with BERT\n",
        "Objective: Explore how the bert-base-uncased tokenizer prepares text for model input.\n",
        "\n",
        "Instructions:\n",
        "1. (Optional) Install the required libraries.\n",
        "2. Load the tokenizer, craft a sample sentence, and encode it with padding plus truncation.\n",
        "3. Print the tokens next to their integer IDs and flag the special tokens.\n",
        "4. Inspect the attention mask to see how padding is hidden from the model.\n",
        "\n",
        "Deliverables:\n",
        "- TODO: Provide the printed list of tokens and IDs with [CLS]/[SEP]/[PAD] highlighted.\n",
        "- TODO: Document the padding choice you made and why it fits the sentence length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "950dde59",
      "metadata": {
        "id": "950dde59"
      },
      "outputs": [],
      "source": [
        "# Optional setup: install dependencies if they are missing in your environment.\n",
        "%pip install -q transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d631bc25",
      "metadata": {
        "id": "d631bc25"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "sample_sentence = \"TODO: replace with a short sentence you want to tokenize\"\n",
        "print(sample_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da3ac84",
      "metadata": {
        "id": "3da3ac84"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer(\n",
        "    sample_sentence,\n",
        "    add_special_tokens=True,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=24,  # TODO: adjust if your sentence needs more room\n",
        "    return_attention_mask=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "input_ids = encoding[\"input_ids\"][0].tolist()\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(\"index | token        | id\")\n",
        "print(\"-------------------------\")\n",
        "for idx, (token, token_id) in enumerate(zip(tokens, input_ids)):\n",
        "    print(f\"{idx:>5} | {token:<12} | {token_id:>5}\")\n",
        "\n",
        "print(\"\\nAttention mask:\", encoding[\"attention_mask\"][0].tolist())\n",
        "special_positions = [(i, tok) for i, tok in enumerate(tokens) if tok in tokenizer.all_special_tokens]\n",
        "print(\"Special tokens (index, token):\", special_positions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a19d64b0",
      "metadata": {
        "id": "a19d64b0"
      },
      "source": [
        "### Exercise 1 reflection\n",
        "- TODO: Describe how [CLS] and [SEP] behave inside the encoder.\n",
        "- TODO: Explain how the attention mask hides padded positions from self-attention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8acfa095",
      "metadata": {
        "id": "8acfa095"
      },
      "source": [
        "## Exercise 2 - Sentiment analysis pipeline\n",
        "Objective: Use a pretrained DistilBERT sentiment pipeline to classify a sentence.\n",
        "\n",
        "Instructions:\n",
        "1. Import the `pipeline` helper from transformers.\n",
        "2. Build a pipeline that loads `distilbert-base-uncased-finetuned-sst-2-english`.\n",
        "3. Pass in a sentence and review the predicted label and score.\n",
        "\n",
        "Deliverables:\n",
        "- TODO: Record the sentence you tested.\n",
        "- TODO: Capture the label plus confidence score and interpret the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0418b34",
      "metadata": {
        "id": "e0418b34"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_pipeline = pipeline(\n",
        "    task=\"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "\n",
        "sentence = \"TODO: add a sentence whose sentiment you want to test\"\n",
        "prediction = sentiment_pipeline(sentence)\n",
        "prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0caf203",
      "metadata": {
        "id": "d0caf203"
      },
      "source": [
        "### Exercise 2 reflection\n",
        "- TODO: Does the predicted label match your expectation? Why or why not?\n",
        "- TODO: How confident is the model and what does the score tell you?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7317d74",
      "metadata": {
        "id": "e7317d74"
      },
      "source": [
        "## Exercise 3 - Custom sentiment analyzer class\n",
        "Objective: Rebuild the pipeline manually so you control tokenization, tensors, and scoring.\n",
        "\n",
        "Instructions:\n",
        "1. Import `AutoTokenizer` and `AutoModelForSequenceClassification`.\n",
        "2. Implement `BERTSentimentAnalyzer` with methods for initialization, preprocessing, and prediction.\n",
        "3. Test the class with multiple sentences.\n",
        "\n",
        "Hints:\n",
        "- Keep a `max_length` attribute so you can reuse it while tokenizing.\n",
        "- Apply `torch.softmax` to transform logits into probabilities.\n",
        "- Return both the label and the probability for clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc452e7",
      "metadata": {
        "id": "7fc452e7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from typing import Dict\n",
        "\n",
        "class BERTSentimentAnalyzer:\n",
        "    def __init__(self, model_name: str = \"distilbert-base-uncased-finetuned-sst-2-english\", max_length: int = 128):\n",
        "        '''TODO: load the tokenizer/model and move the model to the proper device.'''\n",
        "        raise NotImplementedError(\"Initialize tokenizer, model, and device here.\")\n",
        "\n",
        "    def preprocess(self, text: str) -> Dict[str, torch.Tensor]:\n",
        "        '''TODO: clean the text, tokenize, and return tensors ready for inference.'''\n",
        "        raise NotImplementedError(\"Return a dict of tensors produced by the tokenizer.\")\n",
        "\n",
        "    def predict(self, text: str) -> Dict[str, float]:\n",
        "        '''TODO: run a forward pass, apply softmax, and return a label plus probability.'''\n",
        "        raise NotImplementedError(\"Add inference and post-processing logic.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d7cc82",
      "metadata": {
        "id": "f0d7cc82"
      },
      "outputs": [],
      "source": [
        "# TODO: instantiate your analyzer and test several sentences once the class is ready.\n",
        "# analyzer = BERTSentimentAnalyzer()\n",
        "# samples = [\n",
        "#     \"TODO: add a clearly positive statement\",\n",
        "#     \"TODO: add a clearly negative statement\"\n",
        "# ]\n",
        "# for text in samples:\n",
        "#     print(text)\n",
        "#     print(analyzer.predict(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10669267",
      "metadata": {
        "id": "10669267"
      },
      "source": [
        "## Exercise 4 - BERT for Named Entity Recognition\n",
        "Objective: Build a lightweight class that runs a token-classification model and maps tokens to entity labels.\n",
        "\n",
        "Instructions:\n",
        "1. Import `AutoTokenizer` and `AutoModelForTokenClassification`.\n",
        "2. Implement `BERTNamedEntityRecognizer` with init plus a `recognize` method.\n",
        "3. Tokenize sample text, run the model, convert the predictions to entity spans, and test with a short paragraph.\n",
        "\n",
        "Deliverables:\n",
        "- TODO: Return a list of dictionaries like `{text, entity, start, end}` for each detected entity.\n",
        "- TODO: Explain how you handled subword tokens that begin with `##`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa24c9a",
      "metadata": {
        "id": "6aa24c9a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "class BERTNamedEntityRecognizer:\n",
        "    def __init__(self, model_name: str = \"dslim/bert-base-NER\"):\n",
        "        '''TODO: load the tokenizer and model, and detect the available device.'''\n",
        "        raise NotImplementedError(\"Initialize tokenizer, model, and device.\")\n",
        "\n",
        "    def recognize(self, text: str):\n",
        "        '''TODO: tokenize the text, run the model, map predictions to BIO labels, and merge word pieces.'''\n",
        "        raise NotImplementedError(\"Return structured entities.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddabab0a",
      "metadata": {
        "id": "ddabab0a"
      },
      "outputs": [],
      "source": [
        "# TODO: instantiate the recognizer and test it on text that includes people, places, or organizations.\n",
        "# ner = BERTNamedEntityRecognizer()\n",
        "# sample_text = \"TODO: add a short paragraph with at least two entities.\"\n",
        "# ner.recognize(sample_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae59c960",
      "metadata": {
        "id": "ae59c960"
      },
      "source": [
        "## Exercise 5 - Comparing BERT and GPT\n",
        "Objective: Summarize how encoder-style models differ from decoder-style models.\n",
        "\n",
        "Fill the table with concise statements (one line each).\n",
        "\n",
        "| Category | BERT | GPT |\n",
        "|----------|------|-----|\n",
        "| Architecture | TODO | TODO |\n",
        "| Primary purpose | TODO | TODO |\n",
        "| Typical use cases | TODO | TODO |\n",
        "| Strengths | TODO | TODO |\n",
        "| Weaknesses | TODO | TODO |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58dc9313",
      "metadata": {
        "id": "58dc9313"
      },
      "source": [
        "## Exercise 6 - BERT inside Retrieval-Augmented Generation\n",
        "Objective: Explain how BERT-generated embeddings power the retrieval stage of a RAG workflow.\n",
        "\n",
        "Address each bullet with a short paragraph:\n",
        "1. TODO: Describe how BERT encodes queries and documents.\n",
        "2. TODO: Explain how those embeddings are stored and searched in a vector database.\n",
        "3. TODO: Outline how the retrieved passages are handed to a generative model like GPT.\n",
        "4. TODO: Provide a concrete application example (industry or product) where RAG with BERT makes sense.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}